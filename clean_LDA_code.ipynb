{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "import tools\n",
    "import imp\n",
    "from sklearn.metrics import make_scorer\n",
    "import time\n",
    "from sklearn.decomposition import online_lda\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74260\n",
      "74260\n"
     ]
    }
   ],
   "source": [
    "imp.reload(tools)\n",
    "label_encoder,labels = tools.read_labels()\n",
    "total_vec,myCViterator = tools.total_vect_cv_prepare()\n",
    "default_vectorizer,no_filtered_vec = tools.NB_data_prepare()\n",
    "sw = tools.stop_word()\n",
    "lemma_text = tools.read_lemma_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find best set of hyper parameters & get a Linear Regression model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find best hyper parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=38)]: Using backend LokyBackend with 38 concurrent workers.\n",
      "[Parallel(n_jobs=38)]: Done 124 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=38)]: Done 374 tasks      | elapsed: 31.4min\n",
      "[Parallel(n_jobs=38)]: Done 724 tasks      | elapsed: 61.8min\n",
      "[Parallel(n_jobs=38)]: Done 960 out of 960 | elapsed: 80.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vectorizer__selected_model': ('tfidf', {'max_df': 0.5, 'token_pattern': '(?u)\\\\b\\\\d?[a-z]{2,}\\\\d?\\\\b', 'min_df': 10, 'ngram_range': (1, 1)}), 'classifier__selected_model': ('LR_clf', {'C': 1, 'fit_intercept': True, 'class_weight': None})}\n",
      "0.8615001346619984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/tzzhang/.pyenv/versions/3.5.2/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pipeline_helper import PipelineHelper\n",
    "import tools\n",
    "sw = tools.stop_word()\n",
    "lemma_text = tools.read_lemma_data()\n",
    "X = lemma_text\n",
    "Y = labels\n",
    "\n",
    "# A grid search pipeline that includes all combination of vectorizer and classifier\n",
    "pipe = Pipeline([\n",
    "    ('vectorizer', PipelineHelper([\n",
    "        ('count', CountVectorizer(stop_words=sw)),\n",
    "        ('tfidf', TfidfVectorizer(stop_words=sw)),\n",
    "    ])),\n",
    "    ('classifier', PipelineHelper([\n",
    "        ('LR_clf', LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')),\n",
    "    ])),\n",
    "])\n",
    "    \n",
    "params = {\n",
    "    'vectorizer__selected_model': pipe.named_steps['vectorizer'].generate({\n",
    "        'count__ngram_range': [(1,1)],\n",
    "        'count__min_df': [5,10],\n",
    "        'count__max_df': [0.25,0.5,0.75],\n",
    "        'count__token_pattern': [r\"(?u)\\b\\d?[a-z]{2,}\\d?\\b\",r\"(?u)\\b[a-z]{2,}\\b\"],\n",
    "        'tfidf__ngram_range': [(1,1)],\n",
    "        'tfidf__min_df':[5,10],\n",
    "        'tfidf__max_df':[0.25,0.5,0.75],\n",
    "        'tfidf__token_pattern': [r\"(?u)\\b\\d?[a-z]{2,}\\d?\\b\",r\"(?u)\\b[a-z]{2,}\\b\"]\n",
    "    }),\n",
    "    'classifier__selected_model': pipe.named_steps['classifier'].generate({\n",
    "        'LR_clf__fit_intercept': [True,False],\n",
    "        'LR_clf__class_weight': [None,'balanced'],\n",
    "        'LR_clf__C': [1,3],\n",
    "    })\n",
    "}\n",
    "grid = GridSearchCV(pipe, params, scoring='accuracy', verbose=1,cv=5,n_jobs=38)\n",
    "grid.fit(X, Y)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reproduce the best classifier to verifier the result of grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tvectorizer = TfidfVectorizer(max_df = 0.5,min_df=10,stop_words=sw,token_pattern=r'\\b\\d?[a-z]{2,}\\d?\\b',ngram_range=(1,1))\n",
    "Tfidf_vec = Tvectorizer.fit_transform(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74260, 8116)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Tfidf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          Access        0.71      0.55      0.62      2089\n",
      "Delivery of care        0.88      0.94      0.91     10565\n",
      "     Environment        0.80      0.69      0.74       615\n",
      "         Finance        0.89      0.84      0.86      1583\n",
      "\n",
      "         accuracy                           0.86     14852\n",
      "        macro avg       0.82      0.75      0.78     14852\n",
      "     weighted avg       0.86      0.86      0.86     14852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Tfidf_vec, labels, test_size=0.2, random_state=40)\n",
    "best_clf = LogisticRegression(random_state=0, solver='lbfgs',fit_intercept =False,multi_class='multinomial').fit(X_train, y_train)\n",
    "y_pred = best_clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When training the final classifier, all data was used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clf = LogisticRegression(random_state=0, solver='lbfgs',fit_intercept =False,multi_class='multinomial').fit(Tfidf_vec, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write keywords into dummy documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(tools)\n",
    "dummy_list,words_indexes_list = tools.show_word_linear(Cvectorizer,count_vec_per_class,final_clf,label_encoder,des_file = 'dummy_doc_1015_tfidf_exclusive',exclusive=True,norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the best LDA model by custom loss\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = tools.stop_word()\n",
    "FIXED_SEED = 0\n",
    "lemma_text = tools.read_lemma_data()\n",
    "imp.reload(tools)\n",
    "Cvectorizer = CountVectorizer(max_df = 0.5,min_df=10,stop_words=sw,token_pattern=r\"(?u)\\b\\d?[a-z]{2,}\\d?\\b\",ngram_range=(1,1))\n",
    "Cvectorizer.fit_transform(lemma_text)\n",
    "total_vec,myCViterator = tools.total_vect_cv_prepare(vectorizer=Cvectorizer,dummy_document = 'dummy_doc_1015_tfidf_exclusive')\n",
    "Count_vec = total_vec[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73093, 8116)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Count_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_scorer(NB_loss_function)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "my_scorer = make_scorer(tools.NB_loss_function)\n",
    "print(my_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 18 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  16 out of  18 | elapsed: 31.7min remaining:  4.0min\n",
      "[Parallel(n_jobs=30)]: Done  18 out of  18 | elapsed: 31.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "{'learning_decay': 0.9, 'batch_size': 256, 'learning_offset': 256, 'n_components': 40}\n",
      "-8.078364470258512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import online_lda\n",
    "import sklearn\n",
    "imp.reload(sklearn)\n",
    "imp.reload(tools)\n",
    "#max_doc_update_iter=50,learning_decay\n",
    "parameters = {'batch_size':[128,256,512],'learning_offset':[256,64,1024],\n",
    "              'n_components':[40],'learning_decay':[0.8,0.9]}\n",
    "\n",
    "lda_for_gs = online_lda.LatentDirichletAllocation(random_state=40,\\\n",
    "                                           learning_method='online',max_iter=10,n_jobs=1,verbose=1)\n",
    "stop_word_list = []\n",
    "X = total_vec\n",
    "dummy_label = np.array([0,0,0,0])\n",
    "Y=np.hstack([labels,dummy_label])\n",
    "Y = Y[:X.shape[0]]\n",
    "#scoring = my_score\n",
    "# before training/inference:\n",
    "grid = sklearn.model_selection.GridSearchCV(lda_for_gs,param_grid=parameters,verbose=1,cv=myCViterator,scoring = my_scorer,n_jobs=30,refit=True)\n",
    "grid.fit(X,Y)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch will re-train a model according to this hyper-parameter after finding the optimal hyper-parameter, and the loss of the newly trained model will not be exactly equal to the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access : \n",
      "[26 10  6 11 21]\n",
      "[0.06623853 0.06766384 0.0769857  0.35946146 0.24938432]\n",
      "\n",
      "Delivery of care : \n",
      "[33 21 26 34  6]\n",
      "[0.058395   0.1089188  0.36508012 0.07440465 0.0752569 ]\n",
      "\n",
      "Environment : \n",
      "[19 34 37 21 15]\n",
      "[0.00965216 0.10242343 0.56212395 0.03005608 0.27345419]\n",
      "\n",
      "Finance : \n",
      "[21 33 22 27 19]\n",
      "[0.05992373 0.06328937 0.10240512 0.32412602 0.41950788]\n",
      "Entropy Loss: -7.0112849185214605\n",
      "Corr Loss: 0.15202521266924354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8.531537045213895"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_vect = total_vec[-4:]\n",
    "lda = grid.best_estimator_\n",
    "NB_topic_dist = lda.transform(NB_vect)\n",
    "#report_topic = lda2.transform(count_vec)\n",
    "print('Access : ')\n",
    "to_topic = np.argpartition(NB_topic_dist[0],-5)[-5:]\n",
    "print(to_topic)\n",
    "print(NB_topic_dist[0,to_topic])\n",
    "\n",
    "print('\\nDelivery of care : ')\n",
    "to_topic = np.argpartition(NB_topic_dist[1],-5)[-5:]\n",
    "print(to_topic)\n",
    "print(NB_topic_dist[1,to_topic])\n",
    "\n",
    "print('\\nEnvironment : ')\n",
    "to_topic = np.argpartition(NB_topic_dist[2],-5)[-5:]\n",
    "print(to_topic)\n",
    "print(NB_topic_dist[2,to_topic])\n",
    "\n",
    "print('\\nFinance : ')\n",
    "to_topic = np.argpartition(NB_topic_dist[3],-5)[-5:]\n",
    "print(to_topic)\n",
    "print(NB_topic_dist[3,to_topic])\n",
    "\n",
    "tools.NB_loss_function(0,NB_topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 26 appointment, wait, time, clinic, speak, book, clerk, information, contact, explain, phone, office, number, staff, hour, receptionist, doctor, process, schedule, ask, person, leave, procedure, like, cancel, follow, question, appt, date, answer, pcc, advise, booking, happen, request, year, minute, provide, day,\n",
      "\n",
      "Topic 22 care, home, staff, facility, provide, manager, nursing, plan, term, change, case, meeting, dementia, homecare, transition, bath, need, palliative, senior, hc, management, lack, long, support, ltc, caregiver, agent, living, receive, transfer, client, assist, assistance, level, ensure, staffing, live, resident, lodge,\n",
      "\n",
      "Topic 33 nurse, come, triage, ask, leave, staff, room, sit, rude, iv, way, hand, check, comment, talk, unprofessional, shift, desk, nursing, clean, hour, night, treat, manner, look, walk, yell, minute, help, attitude, like, reply, start, wait, male, cry, water, pain, compassion,\n",
      "\n",
      "Topic 32 ed, physician, pain, ct, scan, rdrhc, symptom, emergency, diagnosis, department, chest, diagnose, abdominal, test, assessment, assess, present, treat, attend, hour, doctor, result, severe, discharge, kidney, review, heart, er, history, blood, medical, hospital, send, condition, return, admit, fever, treatment, home,\n",
      "\n",
      "Topic 21 health, centre, emergency, practice, information, standard, department, hospital, worker, social, communication, medical, tooth, style, foothills, south, link, record, dental, community, care, university, lake, dentist, deer, plan, campus, confidentiality, regional, attach, red, healthcare, breach, treatment, uah, mental, sharing, hills, staff,\n",
      "\n",
      "Topic 9 medication, psychiatrist, mental, health, mg, remand, psychiatric, request, prescribe, anxiety, prairie, suicide, psychologist, disorder, depression, ago, form, crisis, suicidal, chronic, grande, addiction, sleep, pain, erc, psychiatry, submit, help, year, prescription, counselor, incarcerate, correctional, centre, hsr, methadone, release, month, dose,\n",
      "\n",
      "Topic 15 antibiotic, injury, ray, infection, left, fracture, wound, leg, orthopedic, foot, right, discharge, arm, follow, fall, home, hip, st, hospital, bone, wrist, eye, sustain, day, ankle, require, assessment, physician, care, shoulder, referral, swollen, august, heal, rehabilitation, week, wecc, stitch, fractured,\n",
      "\n",
      "Topic 1 surgery, surgeon, baby, post, delivery, procedure, surgical, section, breast, day, op, deliver, labour, recovery, biopsy, february, nicu, perform, grey, nuns, feed, pre, cancer, tumor, time, operative, labor, anesthesiologist, week, operating, undergo, complication, obstetrician, operatively, epidural, birth, anesthetic, spinal, bear,\n",
      "\n",
      "Topic 34 discharge, miss, belonging, item, mch, glass, lose, personal, shoe, wallet, allergy, flu, bag, unit, friend, aid, locate, shot, find, allergic, transfer, black, fmc, pharmacy, ms, return, key, clothing, ahe, id, prescription, reaction, clothe, plain, pharmacist, purse, pair, rocky, rmcc,\n",
      "\n",
      "Topic 27 medication, relay, physician, resident, ask, oxygen, death, die, administer, rn, advise, review, morphine, management, drug, communication, deceased, administration, error, discuss, order, partner, manner, provide, discussion, dtr, follow, appropriate, chart, stop, prior, injection, like, jun, question, icu, inappropriate, attend, receive,\n",
      "\n",
      "Topic 11 report, rgh, dialysis, rah, denture, document, forward, outcome, peter, lougheed, unit, relations, royal, lacc, rockyview, general, alexandra, grh, site, pcc, phone, reviewer, feedback, manor, hospitalist, hip, sep, review, renal, questions, lacombe, like, action, secure, discuss, partial, delay, clinic, send,\n",
      "\n",
      "Topic 16 ems, ambulance, crew, transport, paramedic, attendant, female, arrive, claim, driver, metro, scene, drive, personnel, medic, event, house, seizure, elevator, dispatch, male, apparently, fire, emt, police, truck, hospital, car, hit, stretcher, siren, ride, niece, believe, neighbour, lane, think, rude, dispatcher,\n",
      "\n",
      "Topic 17 bed, food, unit, floor, room, meal, chair, bathroom, eat, stretcher, roommate, tray, place, staff, cleanliness, tv, safety, serve, lunch, capacity, breakfast, commode, ocp, space, uah, sleep, fall, bell, ducc, visitor, hospital, use, transfer, housekeeping, privacy, sink, restraint, noise, rest,\n",
      "\n",
      "Topic 18 blood, oncologist, test, cci, lab, fort, result, work, sample, chemotherapy, ultrasound, river, cancer, draw, saskatchewan, chemo, insulin, treatment, april, sugar, diabetic, urine, high, testing, pregnancy, opinion, order, infusion, second, laboratory, level, fscc, pregnant, mcmurray, diabetes, miscarriage, urologist, medical, bloodwork,\n",
      "\n",
      "Topic 29 program, parking, access, service, wheelchair, machine, park, group, building, therapy, zone, available, door, entrance, ticket, outside, spot, main, deny, volunteer, availability, live, use, aish, lot, physio, walk, handicapped, area, doc, unable, central, ot, location, parkade, exit, handicap, stall, limited,\n",
      "\n",
      "Topic 19 private, room, charge, semi, accommodation, sign, request, fee, stay, billing, ward, preferred, waive, isolation, bill, available, unit, form, admit, hospital, night, technician, receive, day, lois, theme, admission, pay, hole, agreement, reverse, rah, adjust, place, blue, discussion, rate, lab, la,\n",
      "\n",
      "Topic 31 bill, pay, invoice, tech, cost, receive, ahs, insurance, card, md, money, therapist, cover, financial, payment, collection, finance, company, credit, billing, dispute, afford, account, coverage, canada, income, receipt, outstanding, ahc, waive, ar, hardship, send, refund, service, visit, receivable, bc, accounts,\n",
      "\n",
      "Topic 25 child, security, parent, stollery, guard, children, smoke, pediatrician, public, school, smoking, aunt, pediatric, policy, cigarette, smoker, ear, hospital, nechc, police, protective, infant, minor, sw, age, enforce, area, stoll, autism, video, phn, vaccination, visitor, services, butt, birth, immunization, property, ban,\n",
      "\n",
      "Topic 30 knee, medicine, hat, physiotherapy, specialist, gi, erc, brace, tylenol, northern, lights, physiotherapist, pain, accord, replacement, regional, mhrh, exercise, session, inmate, express, enema, describe, movement, ip, carewest, vision, constipation, joint, belief, jaw, involved, lump, spine, day, wear, oxycontin, orthotic, splint,\n",
      "\n",
      "Topic 24 stroke, placement, hearing, guardian, ltc, legal, neurologist, aide, speech, grandson, um, abuse, await, westlock, robbins, sl4, neuro, pavilion, slp, egh, rdrh, angiogram, welfare, dgtr, pathologist, loss, slur, rosedale, investigation, battery, sl, waitlist, confused, acute, hanna, cva, team, tia, tc,\n",
      "\n",
      "Topic 28 urgent, file, chumir, sheldon, cardiologist, pcc, close, pursue, prc, wish, norwood, survey, reciev, covenant, records, operations, ucc, uc, paint, columbia, prioritize, british, downtown, reopen, unsatisfied, echocardiogram, director, smchc, richmond, th, corporate, trace, sliver, venipuncture, remedy, rd, john, operational, joseph,\n",
      "\n",
      "Topic 36 catheter, voice, mail, message, law, crutch, insert, brooks, urinary, insertion, foley, common, catheterization, penis, echo, void, catheterize, razor, intake, balloon, trustee, urologist, urethra, indwell, tract, tubing, foreskin, licensed, culture, thousand, suprapubic, vancomycin, urinate, cindy, dispose, audiology, pox, carpet, irrigation,\n",
      "\n",
      "Topic 2 cast, air, throat, ach, boot, sore, fit, strep, nephrologist, employer, splint, ch, swallow, ankle, laptop, break, virus, gnch, slab, pic, barium, donate, swab, aircast, ortho, microwave, nephrology, contrary, crcc, osteoporosis, size, mono, angle, conditioning, pric, tonsil, tight, gland, fitting,\n",
      "\n",
      "Topic 39 line, caller, bruise, nlrhc, form, disappointment, television, photo, ft, woods, rehc, caleo, making, express, kipnes, jb, mill, renfrew, alpha, sunridge, detox, foip, shape, pinch, observed, music, unsupportive, automate, bruising, hide, cls, du, laurier, cable, feature, trusteeship, sweet, electrode, kim,\n",
      "\n",
      "Topic 8 mri, chinook, crh, stocking, kin, reduction, compression, quarantine, regional, diagnostic, fh, server, dislocated, dash, oc, exasperate, lonely, dislocation, privately, reimbursement, altogether, digital, narrow, hopeless, halloween, measles, fitting, isolate, gauge, proceeding, unwanted, wait, complete, urgent, month, shoulder, functional, warmth, timely,\n",
      "\n",
      "Topic 35 grandmother, ring, np, scope, gold, practitioner, stool, granddaughter, grandfather, wedding, sound, grand, ultra, necklace, chain, diamond, sterile, laxative, packing, belt, pendant, grandma, bloody, egccc, kit, diarrhea, grandaughter, reddrc, fcc, sky, softener, copper, circumcision, herpe, standby, color, tail, garment, stomp,\n",
      "\n",
      "Topic 7 walker, officer, peace, camrose, apr, uncle, overflow, foam, bonnyville, peanut, garbage, grove, boyle, spruce, officers, whcc, handcuff, knife, liaison, wetaskawin, anonymous, butter, bashaw, resist, onoway, aged, monie, religion, disagreement, dirt, ground, eps, loan, chaplain, mccauley, agressive, teen, mary, degeneration,\n",
      "\n",
      "Topic 3 plc, swab, operator, contrast, mazankowski, switchboard, housing, lobby, suite, mask, pac, maz, licence, signage, marguerite, masterpiece, stepdaughter, institute, nations, approval, eip, working, attire, emg, terminal, eviction, tighten, racism, precious, rattle, bee, cashier, revoke, ulceration, holistic, guards, offense, african, villa,\n",
      "\n",
      "Topic 20 tube, vehicle, co, bariatric, cpap, nose, motor, km, ng, nasal, apnea, tpn, telehealth, accident, feeding, par, gastric, passenger, everybody, collision, victoria, mvc, landlord, insert, pessary, dislodge, nephrostomy, adhesion, rear, wood, prong, specialty, morbidity, bereavement, damaged, seatbelt, terminology, castor, rec,\n",
      "\n",
      "Topic 37 colonoscopy, website, drcc, valley, drayton, coordination, kaye, healthlink, computer, colon, inefficient, description, screening, advertise, estates, opthalmology, lewis, beleiv, ez, cochrane, summerwood, endocrinology, landing, retirement, mayfair, forcibly, streamline, dire, advertising, residency, fortunately, market, designated, neurologic, rapidly, map, recording, transcribe, center,\n",
      "\n",
      "Topic 6 bladder, appeal, neurosurgeon, radiology, dissatisfaction, permission, arthritis, college, endoscopy, surgeons, aboriginal, rheumatologist, cpsa, gall, physicians, rheumatology, vancouver, polyp, rheumatoid, endometriosis, shunt, camis, banff, overbook, york, racist, instrument, cath, profile, lime, misdiagnosis, ethic, nation, methotrexate, teleconference, scarring, biased, malignant, yellowknife,\n",
      "\n",
      "Topic 10 diet, special, dietician, pco, restriction, dietary, gas, milk, hill, product, als, weigh, celiac, vm, scale, animal, log, lactose, stock, contraindicate, vegetarian, phsycian, cipro, dairy, supplement, dietitian, neice, daugther, protein, cereal, ombudsman, kg, vegan, strict, intolerant, fo, investigator, listing, mayo,\n",
      "\n",
      "Topic 13 shc, free, girlfriend, scent, perfume, gluten, sacred, project, convenient, ipad, religious, ibs, solitary, mclennan, motility, bird, contractor, inefficiency, toll, spiritual, lawn, bead, humane, pray, rainbow, confinement, distribution, gut, tot, forest, neuromuscular, vast, bureaucracy, chapel, mclennon, menopause, society, agecare, juvenile,\n",
      "\n",
      "Topic 12 pcu, bowel, obstruction, fridge, mayerthorpe, southwood, bunk, educator, military, perforation, manning, wants, thicken, miscommunication, choke, prep, hepatology, hemiarthroplasty, sandal, virtual, linger, subcutaneous, pc, dicu, dull, markedly, shaking, spoon, hys, wvhc, supplemental, choking, perforate, fleet, coed, esophageal, bin, clostridium, rah,\n",
      "\n",
      "Topic 14 rash, cream, innisfail, incc, soap, hemorrhoid, itchy, container, skin, cortisone, scabie, dermatologist, ointment, scratch, spread, measle, sm, lotion, pit, eczema, paitent, sunset, thermometer, scalp, topical, diseases, cocnern, canteen, sized, wendy, psoriasis, contagious, genital, miseracordia, torso, patietn, itch, dot, unhooked,\n",
      "\n",
      "Topic 0 mahi, ii, pcm, refusal, fp, queen, broken, faint, complaiant, qe, owner, elizabeth, camp, eehc, unsympathetic, pedway, pact, basket, cutting, crooked, inattentive, enclose, borrow, flutter, transaction, strand, proceedure, suspension, mazenkowski, cabg, locked, kiosk, unassist, qeii, union, suspend, syncope, mailbox, mazankowski,\n",
      "\n",
      "Topic 23 dog, fentanyl, affected, bite, eeg, ob, tubal, ligation, midwife, patch, inspector, gyne, gyn, toxic, cycle, bug, campbell, pateint, rabie, immunocompromis, circulate, midwifery, intensivist, vbac, reassignment, trainer, notation, infestation, strictly, unauthorized, frontline, eve, string, sterilization, bark, environmental, underneath, pushing, bedr,\n",
      "\n",
      "Topic 38 blame, breech, jean, vascular, satisfaction, phlebotomy, abortion, mailing, phlebotomist, compalinant, nod, ass, complaintant, lifeline, supplier, ski, daniel, somebody, cca, samsung, antecubital, army, gruff, prostrate, misunderstand, radway, denie, untie, privacy, buck, brenda, analysis, goodbye, generic, hoodie, vaginally, coma, dnr, flomax,\n",
      "\n",
      "Topic 5 anesthetist, pfic, mirror, po, butterfly, intubation, reserved, rigid, overloaded, million, chipped, uninsured, dayward, oncall, convert, heparin, needle, dental, anesthetize, rhythm, reevaluate, dentist, emphatically, context, laparoscopic, underlying, hysterical, curly, anesthetic, late, pay, calm, relax, undermine, reparative, halfway, anesthesia, improperly, obesity,\n",
      "\n",
      "Topic 4 mate, lloydminster, pca, india, user, meth, oxytocin, lately, al, crystal, anethetist, veterans, fixate, degraded, cursory, invalid, mercy, urinated, navigate, affairs, invasive, production, english, septicemia, naturally, warn, irreversible, sour, emphatically, retaliation, precedence, undermine, stun, pipe, cell, stabbing, chuckle, beautiful, version,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(tools)\n",
    "Count_vec = total_vec[:-4]\n",
    "topics = lda.transform(Count_vec)\n",
    "topic_count = np.sum(topics,axis=0)\n",
    "topic_sort = np.argsort(topic_count)\n",
    "rev_sort = []\n",
    "for i in range(len(topic_sort)-1,-1,-1):\n",
    "    rev_sort.append(topic_sort[i])\n",
    "representative_words = tools.print_topic_word(lda,Cvectorizer,Count_vec,word_num=40,weight_list = rev_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 26: 0.1058 \n",
      "Topic 22: 0.0882 \n",
      "Topic 33: 0.0880 \n",
      "Topic 32: 0.0752 \n",
      "Topic 21: 0.0720 \n",
      "Topic 9: 0.0492 \n",
      "Topic 15: 0.0485 \n",
      "Topic 1: 0.0427 \n",
      "Topic 34: 0.0408 \n",
      "Topic 27: 0.0388 \n",
      "Topic 11: 0.0358 \n",
      "Topic 16: 0.0352 \n",
      "Topic 17: 0.0338 \n",
      "Topic 18: 0.0314 \n",
      "Topic 29: 0.0311 \n",
      "Topic 19: 0.0307 \n",
      "Topic 31: 0.0274 \n",
      "Topic 25: 0.0211 \n",
      "Topic 30: 0.0156 \n",
      "Topic 24: 0.0105 \n",
      "Topic 28: 0.0096 \n",
      "Topic 36: 0.0076 \n",
      "Topic 2: 0.0059 \n",
      "Topic 39: 0.0054 \n",
      "Topic 8: 0.0045 \n",
      "Topic 35: 0.0044 \n",
      "Topic 7: 0.0042 \n",
      "Topic 3: 0.0041 \n",
      "Topic 20: 0.0037 \n",
      "Topic 37: 0.0036 \n",
      "Topic 6: 0.0034 \n",
      "Topic 10: 0.0033 \n",
      "Topic 13: 0.0030 \n",
      "Topic 12: 0.0029 \n",
      "Topic 14: 0.0026 \n",
      "Topic 0: 0.0023 \n",
      "Topic 23: 0.0022 \n",
      "Topic 38: 0.0020 \n",
      "Topic 5: 0.0018 \n",
      "Topic 4: 0.0017 \n"
     ]
    }
   ],
   "source": [
    "topic_count = np.sum(topics,axis=0)\n",
    "topic_count = topic_count / np.sum(topic_count)\n",
    "log = 'Topic {}: {:0.4f} '\n",
    "for i in np.argsort(-topic_count):\n",
    "    print(log.format(i,topic_count[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
